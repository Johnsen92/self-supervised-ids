
@misc{noauthor_self-supervised_2019,
	title = {Self-{Supervised} {Learning}},
	url = {https://www.youtube.com/watch?v=SaJL4SLfrcY&list=PLyKfnf_vpJeD4VfiawfF-oJdsg0APOeY0&index=1},
	abstract = {PAISS 2019
Yann LeCun
New York University 
Facebook AI Research
http://yann.lecun.com},
	urldate = {2020-10-27},
	month = dec,
	year = {2019},
}

@misc{noauthor_lstm_2019,
	title = {{LSTM} is dead. {Long} {Live} {Transformers}!},
	url = {https://www.youtube.com/watch?v=S27pHKBEp30&list=PLyKfnf_vpJeD4VfiawfF-oJdsg0APOeY0&index=2},
	abstract = {Leo Dirac (@leopd) talks about how LSTM models for Natural Language Processing (NLP) have been practically replaced by transformer-based models.  Basic background on NLP, and a brief history of supervised learning techniques on documents, from bag of words, through vanilla RNNs and LSTM.  Then there's a technical deep dive into how Transformers work with multi-headed self-attention, and positional encoding.  Includes sample code for applying these ideas to real-world projects.},
	urldate = {2020-10-27},
	month = dec,
	year = {2019},
}

@article{vaswani_attention_nodate,
	title = {Attention is {All} you {Need}},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
	language = {en},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	pages = {11},
	file = {Vaswani et al. - Attention is All you Need.pdf:/home/jonas/snap/zotero-snap/common/Zotero/storage/U8Y3G5WY/Vaswani et al. - Attention is All you Need.pdf:application/pdf},
}

@article{bonitz_deep_nodate,
	title = {Deep {Self}-{Supervised} and {Semi}-{Supervised} {Learning}},
	language = {en},
	author = {Bonitz, Christoph},
	pages = {45},
	file = {Bonitz - Deep Self-Supervised and Semi-Supervised Learning.pdf:/home/jonas/snap/zotero-snap/common/Zotero/storage/DJJ78HLT/Bonitz - Deep Self-Supervised and Semi-Supervised Learning.pdf:application/pdf},
}

@article{mirsky_kitsune_2018,
	title = {Kitsune: {An} {Ensemble} of {Autoencoders} for {Online} {Network} {Intrusion} {Detection}},
	shorttitle = {Kitsune},
	url = {http://arxiv.org/abs/1802.09089},
	abstract = {Neural networks have become an increasingly popular solution for network intrusion detection systems (NIDS). Their capability of learning complex patterns and behaviors make them a suitable solution for differentiating between normal trafﬁc and network attacks. However, a drawback of neural networks is the amount of resources needed to train them. Many network gateways and routers devices, which could potentially host an NIDS, simply do not have the memory or processing power to train and sometimes even execute such models. More importantly, the existing neural network solutions are trained in a supervised manner. Meaning that an expert must label the network trafﬁc and update the model manually from time to time.},
	language = {en},
	urldate = {2020-10-27},
	journal = {arXiv:1802.09089 [cs]},
	author = {Mirsky, Yisroel and Doitshman, Tomer and Elovici, Yuval and Shabtai, Asaf},
	month = may,
	year = {2018},
	note = {arXiv: 1802.09089},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	annote = {Comment: Appears in Network and Distributed Systems Security Symposium (NDSS) 2018},
	file = {Mirsky et al. - 2018 - Kitsune An Ensemble of Autoencoders for Online Ne.pdf:/home/jonas/snap/zotero-snap/common/Zotero/storage/I34S4RPI/Mirsky et al. - 2018 - Kitsune An Ensemble of Autoencoders for Online Ne.pdf:application/pdf},
}

@inproceedings{tan_neural_2019,
	address = {Osnabrueck, Germany},
	title = {A {Neural} {Attention} {Model} for {Real}-{Time} {Network} {Intrusion} {Detection}},
	isbn = {978-1-72811-028-8},
	url = {https://ieeexplore.ieee.org/document/8990890/},
	doi = {10.1109/LCN44214.2019.8990890},
	abstract = {The diversity and ever-evolving nature of network intrusion attacks has made defense a real challenge for security practitioners. Recent research in the domain of Network-based Intrusion Detection System has mainly focused on adopting a ﬂow-based approach when extracting features from raw packets. One drawback of this is that attack detection can only be carried out after the ﬂow has ended. In this work, we present a new technique based on the neural attention mechanism; unlike many existing solutions, our technique can be applied for realtime attack detection since it uses time slot-based features. The proposed solution is a modiﬁed version of the transformer model which has been proposed and used in the language translation domain. We conduct experiments on a dataset extracted from a recent repository network trafﬁc containing several kinds of network attack. We use the “bidirectional LSTM” and “conditional random ﬁelds” models as baseline for comparison and our performance results demonstrate that the proposed solution signiﬁcantly outperforms the two baselines in terms of precision, recall, and false positive rates. In addition, we show that our solution is more computationally efﬁcient than the bidirectional LSTM model as a result of the removal of recurrent layers.},
	language = {en},
	urldate = {2020-10-27},
	booktitle = {2019 {IEEE} 44th {Conference} on {Local} {Computer} {Networks} ({LCN})},
	publisher = {IEEE},
	author = {Tan, Mengxuan and Iacovazzi, Alfonso and Cheung, Ngai-Man Man and Elovici, Yuval},
	month = oct,
	year = {2019},
	pages = {291--299},
	file = {Tan et al. - 2019 - A Neural Attention Model for Real-Time Network Int.pdf:/home/jonas/snap/zotero-snap/common/Zotero/storage/MIINUEQL/Tan et al. - 2019 - A Neural Attention Model for Real-Time Network Int.pdf:application/pdf},
}

@article{ko_self-supervised_2020,
	title = {Self-supervised network traffic management for {DDoS} mitigation within the {ISP} domain},
	volume = {112},
	issn = {0167739X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167739X20302193},
	doi = {10.1016/j.future.2020.06.002},
	abstract = {The continuing development of 5G technology increases the number of devices connected to the internet, this provides an increasing potential for cybercriminals to orchestrate detrimental Distributed Denial of Service (DDoS) attacks. The research community continues to develop new techniques to respond to the growing demand for DDoS mitigation. The internet service provider (ISP) provides internet access for users, so the attack traffic arrives at this location before reaching the victim. Deploying the mitigation system within the ISP domain offers an efficient solution. Therefore, we propose a dynamic network traffic managing (DNTM) system, which encompasses an Attack Detector, an IP Prioritiser, a Traffic Manager, and a Netflow Classifier, for the ISP. The IP prioritiser categorises IP addresses into normal and suspicious classes. The Traffic Manager makes use of the existing ISP mechanisms including ingress \& egress filtering, rate limiting, blackholing and normal routing to take different mitigation actions. The Netflow Classifier is a hybrid ensemble model that utilises both unsupervised and supervised learning techniques. The classifier employs two self-organising maps (SOMs) to label data to train a supervised ensemble unit, which includes Random Forests, Decision Trees, and Gradient Boosted Trees (SRDG), to get the final classification. The Netflow Classifier achieved over 96\% average on recall, precision and F1 score on UDP flood, ICMP flood and TCP flood attack data sets.},
	language = {en},
	urldate = {2020-10-27},
	journal = {Future Generation Computer Systems},
	author = {Ko, Ili and Chambers, Desmond and Barrett, Enda},
	month = nov,
	year = {2020},
	pages = {524--533},
	file = {Ko et al. - 2020 - Self-supervised network traffic management for DDo.pdf:/home/jonas/snap/zotero-snap/common/Zotero/storage/RTKILCBV/Ko et al. - 2020 - Self-supervised network traffic management for DDo.pdf:application/pdf},
}

@article{trinh_selfie_2019,
	title = {Selfie: {Self}-supervised {Pretraining} for {Image} {Embedding}},
	shorttitle = {Selfie},
	url = {http://arxiv.org/abs/1906.02940},
	abstract = {We introduce a pretraining technique called Selﬁe, which stands for SELFsupervised Image Embedding. Selﬁe generalizes the concept of masked language modeling of BERT (Devlin et al., 2019) to continuous data, such as images, by making use of the Contrastive Predictive Coding loss (Oord et al., 2018). Given masked-out patches in an input image, our method learns to select the correct patch, among other “distractor” patches sampled from the same image, to ﬁll in the masked location. This classiﬁcation objective sidesteps the need for predicting exact pixel values of the target patches. The pretraining architecture of Selﬁe includes a network of convolutional blocks to process patches followed by an attention pooling network to summarize the content of unmasked patches before predicting masked ones. During ﬁnetuning, we reuse the convolutional weights found by pretraining. We evaluate Selﬁe on three benchmarks (CIFAR-10, ImageNet 32 × 32, and ImageNet 224 × 224) with varying amounts of labeled data, from 5\% to 100\% of the training sets. Our pretraining method provides consistent improvements to ResNet-50 across all settings compared to the standard supervised training of the same network. Notably, on ImageNet 224 × 224 with 60 examples per class (5\%), our method improves the mean accuracy of ResNet-50 from 35.6\% to 46.7\%, an improvement of 11.1 points in absolute accuracy. Our pretraining method also improves ResNet-50 training stability, especially on low data regime, by signiﬁcantly lowering the standard deviation of test accuracies across different runs.},
	language = {en},
	urldate = {2020-10-27},
	journal = {arXiv:1906.02940 [cs, eess, stat]},
	author = {Trinh, Trieu H. and Luong, Minh-Thang and Le, Quoc V.},
	month = jul,
	year = {2019},
	note = {arXiv: 1906.02940},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing, Statistics - Machine Learning},
	file = {Trinh et al. - 2019 - Selfie Self-supervised Pretraining for Image Embe.pdf:/home/jonas/snap/zotero-snap/common/Zotero/storage/2VFN54FV/Trinh et al. - 2019 - Selfie Self-supervised Pretraining for Image Embe.pdf:application/pdf},
}

@inproceedings{chen_hybrid_2020,
	title = {A {Hybrid} {Feature} {Extraction} {Network} for {Intrusion} {Detection} {Based} on {Global} {Attention} {Mechanism}},
	doi = {10.1109/CIBDA50819.2020.00114},
	abstract = {The widespread application of 5G will make intrusion detection of large-scale network traffic a mere need. However, traditional intrusion detection cannot meet the requirements by manually extracting features, and the existing AI methods are also relatively inefficient. Therefore, when performing intrusion detection tasks, they have significant disadvantages of high false alarm rates and low recognition performance. For this challenge, this paper proposes a novel hybrid network, RULA-IDS, which can perform intrusion detection tasks by great amount statistical data from the network monitoring system. RULA-IDS consists of the fully connected layer, the feature extraction layer, the global attention mechanism layer and the SVM classification layer. In the feature extraction layer, the residual U-Net and LSTM are used to extract the spatial and temporal features of the network traffic attributes. It is worth noting that we modified the structure of U-Net to suit the intrusion detection task. The global attention mechanism layer is then used to selectively retain important information from a large number of features and focus on those. Finally, the SVM is used as a classifier to output results. The experimental results show that our method outperforms existing state-of-the-art intrusion detection methods, and the accuracies of training and testing are improved to 97.01\% and 98.19\%, respectively, and presents stronger robustness during training and testing.},
	booktitle = {2020 {International} {Conference} on {Computer} {Information} and {Big} {Data} {Applications} ({CIBDA})},
	author = {Chen, W. and Cao, H. and Lv, X. and Cao, Y.},
	month = apr,
	year = {2020},
	keywords = {AI methods, computer network security, Deep Learning, false alarm rates, Feature extraction, feature extraction layer, fully connected layer, Global Attention Mechanism, global attention mechanism layer, hybrid feature extraction network, hybrid network, Intrusion detection, Intrusion Detection, intrusion detection task, intrusion detection tasks, large-scale network traffic, low recognition performance, Machine learning, network monitoring system, network traffic attributes, neural nets, pattern classification, Residual U Net, RULA-IDS, spatial features, support vector machines, Support vector machines, SVM classification layer, telecommunication traffic, Telecommunication traffic, temporal features, Testing, Training},
	pages = {481--485},
	file = {IEEE Xplore Full Text PDF:/home/jonas/snap/zotero-snap/common/Zotero/storage/5U8QW8WY/Chen et al. - 2020 - A Hybrid Feature Extraction Network for Intrusion .pdf:application/pdf;IEEE Xplore Abstract Record:/home/jonas/snap/zotero-snap/common/Zotero/storage/JNAEIKB8/9148571.html:text/html},
}

@inproceedings{khan_improved_2019,
	title = {An {Improved} {Convolutional} {Neural} {Network} {Model} for {Intrusion} {Detection} in {Networks}},
	doi = {10.1109/CCC.2019.000-6},
	abstract = {Network intrusion detection is an important component of network security. Currently, the popular detection technology used the traditional machine learning algorithms to train the intrusion samples, so as to obtain the intrusion detection model. However, these algorithms have the disadvantage of low detection rate. Deep learning is more advanced technology that automatically extracts features from samples. In view of the fact that the accuracy of intrusion detection is not high in traditional machine learning technology, this paper proposes a network intrusion detection model based on convolutional neural network algorithm. The model can automatically extract the effective features of intrusion samples, so that the intrusion samples can be accurately classified. Experimental results on KDD99 datasets show that the proposed model can greatly improve the accuracy of intrusion detection.},
	booktitle = {2019 {Cybersecurity} and {Cyberforensics} {Conference} ({CCC})},
	author = {Khan, R. U. and Zhang, X. and Alazab, M. and Kumar, R.},
	month = may,
	year = {2019},
	keywords = {CNN, Convolution, convolutional neural nets, convolutional neural network algorithm, Convolutional neural networks, Cyber Security, feature extraction, Feature extraction, improved convolutional neural network model, Intrusion detection, Intrusion Detection, intrusion samples, KDD99 datasets, Kernel, learning (artificial intelligence), low detection rate, Machine learning, network intrusion detection model, network security, Network Security, pattern classification, popular detection technology, security of data, Support vector machines, traditional machine learning technology},
	pages = {74--77},
	file = {IEEE Xplore Full Text PDF:/home/jonas/snap/zotero-snap/common/Zotero/storage/SSXVJS5B/Khan et al. - 2019 - An Improved Convolutional Neural Network Model for.pdf:application/pdf;IEEE Xplore Abstract Record:/home/jonas/snap/zotero-snap/common/Zotero/storage/9CNUKIG7/8854549.html:text/html},
}

@misc{cic_ids2018,
	title = {{IDS} 2018 {\textbar} {Datasets} {\textbar} {Research} {\textbar} {Canadian} {Institute} for {Cybersecurity} {\textbar} {UNB}},
	url = {https://www.unb.ca/cic/datasets/ids-2018.html},
	abstract = {Datasets by CIC and ISCX are used around the world for security testing and malware prevention., Datasets by CIC and ISCX are used around the world for security testing and malware prevention.},
	language = {en},
	urldate = {2020-11-03},
	file = {Snapshot:/home/jonas/snap/zotero-snap/common/Zotero/storage/2SBEBKM4/ids-2018.html:text/html},
}
