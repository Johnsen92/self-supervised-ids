\begin{table}[htb]
    \centering
    \begin{tabular}{@{}cccc@{}}
        \toprule
         &  NONE &  MASK &  OBSCURE \\
        \midrule
        Hyperparameters &  &  &  \\
        Epochs Supervised &  200 &  200 &  200 \\
        Epochs Pretraining &  200 &  10 &  10 \\
        Batch size &  1024 &  1024 &  1024 \\
        Proxy task &  NONE &  MASK &  OBSCURE \\
        Pretraining percentage &  0.00 \% &  80.00 \% &  80.00 \% \\
        Training percentage &  1.00 \% &  1.00 \% &  1.00 \% \\
        Validation percentage &  10.00 \% &  10.00 \% &  10.00 \% \\
        Specialized subset &   &   &   \\
        Learning rate &  0.001 &  0.001 &  0.001 \\
        Random Seed &  500 &  500 &  500 \\
         \\
        Modelparameters &  &  &  \\
        \# Layers &  10 &  10 &  10 \\
        \# Heads &  3 &  3 &  3 \\
        Forward expansion &  2 &  2 &  2 \\
        Dropout &  0.0 &  0.0 &  0.0 \\
         \\
        Training metrics &  &  &  \\
        Best epoch &  191 &  129 &  197 \\
        Time to best epoch &  1h 1m &  0h 41m &  1h 2m \\
         \\
        Performance metrics &  &  &  \\
        Accuracy &  98.328 \% &  98.165 \% &  98.256 \% \\
        False alarm rate &  27.210 \% &  33.792 \% &  28.912 \% \\
        Missed alarm rate &  15.788 \% &  1.740 \% &  14.447 \% \\
        Detection rate &  84.212 \% &  98.260 \% &  85.553 \% \\
        Precision &  72.790 \% &  66.208 \% &  71.088 \% \\
        Specificity &  98.845 \% &  98.162 \% &  98.722 \% \\
        Recall &  84.212 \% &  98.260 \% &  85.553 \% \\
        F1-Measure &  78.086 \% &  79.111 \% &  77.653 \% \\
        \bottomrule
    \end{tabular}
    \caption{Experiments 3.5.1-6 with transformer encoder model finetuned with 1\% of dataset UNSW-NB15.}
    \label{table:results:lstm:stats_flows15_10}
\end{table}